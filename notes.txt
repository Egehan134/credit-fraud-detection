Created a new env for packages.
Data set is highly unbalanced, frauds are 0.172% of all transactions.
V1-V28: Principal components,
Time: Seconds elapsed since the first transaction.
Amount: Transaction amount
Class: Target variable(1 = fraud, 0 = legitimate)

A larger training set like 80/20 will improve model learning but may reduce evaluation reliability due to a smaller test set.
But in the orther a larger test set like 60/40 gives more reliable evaluation but reduces training data that causes less trained model.

We must use StandardScaler for normalize the range of variables in a dataset. It ensures that all variables contribute equally to the model's learning
process. StandardScaler standardizes columns by transforming them to have a mean of 0 and a standard deviation of 1.(didn't need in this data)

RobustScaler is lesst prone to outliers.

Model               | Advantages                                            | Disadvantages
Logistic Regression | Simple, fast, easy to interpret                       | Requires linear relationships, weak with imbalanced data
SVM                 | Good at modeling complex boundaries                   | Slow, difficult parameter tuning, weak with large data
KNN                 | Simple, easily adapts to new data                     | Slow, struggles with large data, weak with imbalanced data
Random Forest       | Good with imbalanced data, high accuracy              | Difficult to interpret, requires some tuning
XGBoost / LightGBM  | High accuracy, fast, works well with imbalanced data  | Complex tuning, risk of overfitting
Neural Networks     | Excellent with complex data, powerful modeling        | Overfitting, long training times, difficult to interpret
Decision Trees      | Fast, easy to interpret, low computational cost       | Overfitting, weak with imbalanced data

RandomForest Confusion Matrix (wihtout balance between samples)
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     85292
           1       0.96      0.74      0.84       151

    accuracy                           1.00     85443
   macro avg       0.98      0.87      0.92     85443
weighted avg       1.00      1.00      1.00     85443

It seems very good but accuracy must be misleading because the data is highly imbalanced. Normal transactions vastly outnumbers fradulent ones.

For fix the imbalance between samples we can use SMOTE or Sub-Sampling (or two of them)(there are a lot of other methods too)
Aspect                      | SMOTE (Synthetic Minority Over-sampling Technique)                                                            | Sub-sampling (Under-sampling)
Purpose                     | Increases the number of minority class samples by generating synthetic examples.                              | Reduces the number of majority class samples to balance the classes.
Approach                    | Over-sampling: Generates synthetic examples for the minority class.                                           | Under-sampling: Removes examples from the majority class.
Impact on Data              | Adds synthetic data points to the minority class.                                                             | Reduces the total number of data points by removing majority class examples.
Risk of Information Loss    | No loss of real data since new synthetic examples are created.                                                | Potential loss of valuable data from the majority class.
Effect on Model Performance | Helps the model learn the minority class better, but may risk overfitting with too many synthetic examples.   | Can lead to faster training and lower memory usage but may affect model's ability to generalize due to loss of majority class data.
Ideal Scenario              | When the minority class is significantly underrepresented and you want to increase its representation.        | When the majority class is overwhelmingly large and may dominate the model's learning process.
Handling Data Imbalance     | Balances data by creating more instances of the minority class.                                               | Balances data by reducing the number of majority class instances.
Model Complexity            | May increase model complexity due to the creation of synthetic data.                                          | May decrease model complexity by reducing the dataset size.
Overfitting Risk            | Higher risk if too many synthetic examples are created.                                                       | Lower risk of overfitting, but may lead to poor model performance due to insufficient data from the majority class.

RandomForest Confusion Matrix (with balance between samples)
             precision    recall  f1-score   support

           0       1.00      1.00      1.00     85292
           1       0.96      0.74      0.84       151

    accuracy                           1.00     85443
   macro avg       0.98      0.87      0.92     85443
weighted avg       1.00      1.00      1.00     85443

Accuracy:
Classifier: Logistic Regression has an average training accuracy of 93.89%
Classifier: K-Nearest Neighbors has an average training accuracy of 91.72%
Classifier: Support Vector Machine has an average training accuracy of 92.59%
Classifier: Decision Tree has an average training accuracy of 91.13%
Classifier: RandomForestClassifier has an average training accuracy of 93.31%

Cross Validation Scores after Hyperparameter Tuning:
Logistic Regression: 93.89% accuracy
K-Nearest Neighbors: 92.0% accuracy
Support Vector Classifier: 94.18% accuracy
Decision Tree Classifier: 91.28% accuracy
Random Forest Classifier: 93.31% accuracy